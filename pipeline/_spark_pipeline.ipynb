{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Imports\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# PySpark ML Imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Parsing and requests Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Other Imports\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# System paths\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Database path\n",
    "DATABASE_PATH = \"../database/DDBB_duckdb.duckdb\"\n",
    "\n",
    "# Competition URL paths\n",
    "login_url = 'http://big-data-competitions.swedencentral.cloudapp.azure.com:8080/auth/login'\n",
    "upload_url = 'http://big-data-competitions.swedencentral.cloudapp.azure.com:8080/competitions/imdb/submit'\n",
    "submissions_url = 'http://big-data-competitions.swedencentral.cloudapp.azure.com:8080/submissions/'\n",
    "\n",
    "\n",
    "# Credentials for authentication\n",
    "username = 'group25'\n",
    "password = '6XwJgJRh'\n",
    "\n",
    "# Path to your val_result.csv and test_result.csv files\n",
    "val_csv_path = 'val_result.csv'\n",
    "test_csv_path = 'test_result.csv'\n",
    "\n",
    "\n",
    "def fetch_duckdb() -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetches all the required data from the database and returns an array of dataframes.\n",
    "    TEMP: Only data from the movies table is being fetched right now. Expand to writers\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=DATABASE_PATH, read_only=False)\n",
    "    df = con.execute('''\n",
    "    WITH director_avg_scores AS (\n",
    "        SELECT \n",
    "            d.director_id,\n",
    "            COALESCE(SUM(CASE WHEN m.label THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0.5) AS director_avg_score\n",
    "        FROM \n",
    "            directing d\n",
    "        INNER JOIN \n",
    "            movies m ON d.movie_id = m.movie_id\n",
    "        WHERE \n",
    "            m.subset = 'train'\n",
    "        GROUP BY \n",
    "            d.director_id\n",
    "    ),\n",
    "    director_scores AS (\n",
    "        SELECT \n",
    "            d.movie_id,\n",
    "            COUNT(d.director_id) AS director_count,\n",
    "            AVG(COALESCE(das.director_avg_score, 0.5)) AS director_avg_score\n",
    "        FROM \n",
    "            directing d\n",
    "        LEFT JOIN \n",
    "            director_avg_scores das ON das.director_id = d.director_id\n",
    "        GROUP BY \n",
    "            d.movie_id\n",
    "    ),\n",
    "    writer_avg_scores AS (\n",
    "        SELECT \n",
    "            w.writer_id,\n",
    "            COALESCE(SUM(CASE WHEN m.label THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0.5) AS writer_avg_score\n",
    "        FROM \n",
    "            writing w\n",
    "        INNER JOIN \n",
    "            movies m ON w.movie_id = m.movie_id\n",
    "        WHERE \n",
    "            m.subset = 'train'\n",
    "        GROUP BY \n",
    "            w.writer_id\n",
    "    ),\n",
    "    writer_scores AS (\n",
    "        SELECT \n",
    "            w.movie_id,\n",
    "            COUNT(w.writer_id) AS writer_count,\n",
    "            AVG(COALESCE(was.writer_avg_score, 0.5)) AS writer_avg_score\n",
    "        FROM \n",
    "            writing w\n",
    "        LEFT JOIN \n",
    "            writer_avg_scores was ON w.writer_id = was.writer_id\n",
    "        GROUP BY \n",
    "            w.movie_id\n",
    "    ),\n",
    "    numbered_movies AS (\n",
    "        SELECT\n",
    "            m.*,\n",
    "            ROW_NUMBER() OVER () AS row_num\n",
    "        FROM \n",
    "            movies m\n",
    "    )\n",
    "    SELECT\n",
    "        nm.subset, \n",
    "        nm.movie_id,\n",
    "        nm.num_votes,\n",
    "        nm.runtime_min,\n",
    "        nm.title_length,\n",
    "        COALESCE(ds.director_avg_score, 0.5) AS director_avg_score,\n",
    "        COALESCE(ds.director_count, 0) AS director_count,\n",
    "        CASE WHEN nm.label THEN 1 ELSE 0 END AS label,\n",
    "        nm.label AS label_og,\n",
    "        COALESCE(ws.writer_avg_score, 0.5) AS writer_avg_score,\n",
    "        COALESCE(ws.writer_count, 0) AS writer_count\n",
    "    FROM \n",
    "        numbered_movies nm\n",
    "    LEFT JOIN \n",
    "        director_scores ds ON nm.movie_id = ds.movie_id\n",
    "    LEFT JOIN \n",
    "        writer_scores ws ON nm.movie_id = ws.movie_id\n",
    "    ORDER BY\n",
    "        nm.row_num;\n",
    "    ''').fetch_df()\n",
    "    con.close()\n",
    "    \n",
    "    \n",
    "    train = df[df['subset'] == 'train'].drop(['subset'], axis=1).dropna()\n",
    "    test = df[df['subset'] == 'test'].drop(['subset', 'label'], axis=1)\n",
    "    validation = df[df['subset'] == 'val'].drop(['subset', 'label'], axis=1)\n",
    "    \n",
    "    return train, test, validation\n",
    "\n",
    "def generate_pipeline(features: list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Function to generate the Spark pipeline based on the following operations:\n",
    "        - Assembling (choosing) the desired features (numeric).\n",
    "        - Index the selected features to be processed by the pipeline (strings).\n",
    "        - Initializing the pipeline based on the indexed features.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    pipeline = Pipeline().setStages([assembler])\n",
    "    return pipeline\n",
    "\n",
    "def generate_output_pipeline(features: list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Function to generate the Spark pipeline based on the following operations:\n",
    "        - Assembling (choosing) the desired features (numeric).\n",
    "        - Index the selected features to be processed by the pipeline (strings).\n",
    "        - Initializing the pipeline based on the indexed features.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    pipeline = Pipeline().setStages([assembler])\n",
    "    return pipeline\n",
    "\n",
    "def hyper_parameter_tuning(prepared: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Function to find the best hyperparameters for a RandomForestClassifier\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(rf.numTrees, [100, 200, 300])  # Number of trees in the forest\n",
    "                 .addGrid(rf.maxDepth, [2, 5, 10, 15])    # Maximum depth of each tree\n",
    "                 .addGrid(rf.maxBins, [5, 10, 20, 32])\n",
    "                 .build())\n",
    "    \n",
    "    # Define evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    \n",
    "    # Define cross-validation\n",
    "    cv = CrossValidator(estimator=rf,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)  # Use 5 folds\n",
    "    \n",
    "    # Train model using cross-validation\n",
    "    cv_model = cv.fit(prepared)\n",
    "    \n",
    "    # Best model from cross-validation\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    best_max_depth = best_model._java_obj.getMaxDepth()\n",
    "    best_num_trees = best_model._java_obj.getNumTrees()\n",
    "    best_max_bins = best_model._java_obj.getMaxBins()\n",
    "    print(\"Best maxDepth:\", best_max_depth)\n",
    "    print(\"Best numTrees:\", best_num_trees)\n",
    "    print(\"Best maxBins:\", best_max_bins)\n",
    "    \n",
    "def create_submission(model, validation, test, features) -> None:\n",
    "    \"\"\"\n",
    "    Create the required submission file in .csv format\n",
    "    \n",
    "    :param model: PySpark generated binary classifier\n",
    "    \"\"\"    \n",
    "    pipeline = generate_output_pipeline(features)\n",
    "    pipeline_fit = pipeline.fit(validation)\n",
    "    p_val = pipeline_fit.transform(validation)\n",
    "    p_test = pipeline_fit.transform(test)\n",
    "    \n",
    "    val_results = model.transform(p_val).select('prediction').toPandas()\n",
    "    test_results = model.transform(p_test).select('prediction').toPandas()\n",
    "\n",
    "    # Cast to bool and store in .csv\n",
    "    val_results.astype(bool).to_csv(\"val_result.csv\", index=False, header=None)\n",
    "    test_results.astype(bool).to_csv(\"test_result.csv\", index=False, header=None)\n",
    "    \n",
    "def automated_submission_online() -> None:\n",
    "    \"\"\"\n",
    "    Uploads the CSV files to the web server and prints the most recent score and the best score\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    # Create a session\n",
    "    session = requests.Session()\n",
    "\n",
    "    # Login to the website\n",
    "    login_data = {\n",
    "        'username': username,\n",
    "        'password': password\n",
    "    }\n",
    "\n",
    "    login_response = session.post(login_url, data=login_data)\n",
    "\n",
    "    # Check if login was successful (you may need to adjust this based on the website's response)\n",
    "    if login_response.status_code == 200:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Login failed. Status code:\", login_response.status_code)\n",
    "        exit()\n",
    "\n",
    "    # Create a dictionary containing the files to be uploaded\n",
    "    files = {\n",
    "        'valid': open(val_csv_path, 'rb'),\n",
    "        'test': open(test_csv_path, 'rb')\n",
    "    }\n",
    "\n",
    "    # Make the POST request to upload the files\n",
    "    upload_response = session.post(upload_url, files=files)\n",
    "\n",
    "    # Check if the upload was successful\n",
    "    if upload_response.status_code == 200:\n",
    "        # Parse the HTML response\n",
    "        soup = BeautifulSoup(upload_response.text, 'html.parser')\n",
    "        # Find the submission made by \"group25\"\n",
    "        submission_rows = soup.find_all('tr', class_='submission')\n",
    "        # Iterate through each submission row\n",
    "        for row in submission_rows:\n",
    "            # Find the <th> tag with scope=\"row\" and text equal to \"group25\" within the current row\n",
    "            submission_group = row.find('th', scope='row', string='group25')\n",
    "            \n",
    "            # Check if the submission_group is found in the current row\n",
    "            if submission_group:\n",
    "                # Extract the submission time and validation score\n",
    "                submission_time = row.find('td').get_text()\n",
    "                validation_score = submission_group.find_next_sibling('td').get_text()\n",
    "                \n",
    "                # Print the most recent submission for \"group25\"\n",
    "                print(f\"Best submission by group25: Time: {submission_time}, Validation Score: {validation_score}\")\n",
    "                # Once we find the submission, we can break out of the loop\n",
    "                break\n",
    "        else:\n",
    "            # If no submission for \"group25\" is found in any of the rows\n",
    "            print(\"Submission by group25 not found.\")\n",
    "            \n",
    "        # Send a GET request to the submissions page\n",
    "        submissions_response = session.get(submissions_url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if submissions_response.status_code == 200:\n",
    "            # Parse the HTML response\n",
    "            soup = BeautifulSoup(submissions_response.text, 'html.parser')\n",
    "            # Find all submission rows\n",
    "            submission_rows = soup.find_all('tr', class_='submission')\n",
    "            if submission_rows:\n",
    "                # Extract information from the most recent submission\n",
    "                most_recent_submission = submission_rows[0]  # The first row is the most recent one\n",
    "                submission_time = most_recent_submission.find('td')\n",
    "                validation_score = submission_time.find_next_sibling('td')\n",
    "                print(f\"Most recent submission by group25: Time: {submission_time.get_text()}, Validation Score: {validation_score.get_text()}\")\n",
    "            else:\n",
    "                print(\"No submissions found.\")   \n",
    "        else:\n",
    "            print(\"Submission by group25 not found.\")\n",
    "    else:\n",
    "        print(\"Failed to upload files. Status code:\", upload_response.status_code)\n",
    "\n",
    "    \n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main PySpark pipeline execution.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    # Initialize PySpark Context\n",
    "    conf = SparkConf().setAppName(\"binary-ml-classification\")\n",
    "    sc = SparkContext.getOrCreate(conf)\n",
    "    sqlContext = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Fetch data and process features to obtain a Spark Dataframe\n",
    "    train, test, validation = fetch_duckdb()\n",
    "    features = [\"runtime_min\", \"num_votes\", \"director_avg_score\", \"director_count\", \"writer_avg_score\", \"writer_count\"]\n",
    "    df_train = sqlContext.createDataFrame(train)\n",
    "    \n",
    "    # Generate the pipeline\n",
    "    pipeline = generate_pipeline(features)\n",
    "    \n",
    "    # Fit the pipeline using the Spark Dataframe\n",
    "    pipeline_fit = pipeline.fit(df_train)  \n",
    "    \n",
    "    # Generate and train the model\n",
    "    prepared = pipeline_fit.transform(df_train)\n",
    "\n",
    "    # Run if need to tune hyperparameters\n",
    "    # hyper_parameter_tuning(prepared)\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth = 10, numTrees=300, maxBins=20)\n",
    "    best_model = rf.fit(prepared)\n",
    "    \n",
    "    \n",
    "    # # Read output generation files\n",
    "    df_validation = sqlContext.createDataFrame(validation)\n",
    "    df_test = sqlContext.createDataFrame(test)\n",
    "\n",
    "    create_submission(best_model, df_validation, df_test, features)\n",
    "    automated_submission_online()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigDataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
