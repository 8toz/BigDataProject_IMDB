{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Imports\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext, DataFrame\n",
    "\n",
    "# PySpark ML Imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml.feature import Bucketizer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Other Imports\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# System paths\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Database path\n",
    "DATABASE_PATH = \"../database/DDBB_duckdb.duckdb\"\n",
    "\n",
    "\n",
    "def fetch_duckdb() -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetches all the required data from the database and returns an array of dataframes.\n",
    "    TEMP: Only data from the movies table is being fetched right now. Expand to writers\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=DATABASE_PATH, read_only=False)\n",
    "    df = con.execute('''\n",
    "    WITH director_avg_scores AS (\n",
    "        SELECT \n",
    "            d.director_id,\n",
    "            COALESCE(SUM(CASE WHEN m.label THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0.5) AS director_avg_score\n",
    "        FROM \n",
    "            directing d\n",
    "        INNER JOIN \n",
    "            movies m ON d.movie_id = m.movie_id\n",
    "        WHERE \n",
    "            m.subset = 'train'\n",
    "        GROUP BY \n",
    "            d.director_id\n",
    "    ),\n",
    "    director_scores AS (\n",
    "        SELECT \n",
    "            d.movie_id,\n",
    "            COUNT(d.director_id) AS director_count,\n",
    "            AVG(COALESCE(das.director_avg_score, 0.5)) AS director_avg_score\n",
    "        FROM \n",
    "            directing d\n",
    "        LEFT JOIN \n",
    "            director_avg_scores das ON das.director_id = d.director_id\n",
    "        GROUP BY \n",
    "            d.movie_id\n",
    "    ),\n",
    "    writer_avg_scores AS (\n",
    "        SELECT \n",
    "            w.writer_id,\n",
    "            COALESCE(SUM(CASE WHEN m.label THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0.5) AS writer_avg_score\n",
    "        FROM \n",
    "            writing w\n",
    "        INNER JOIN \n",
    "            movies m ON w.movie_id = m.movie_id\n",
    "        WHERE \n",
    "            m.subset = 'train'\n",
    "        GROUP BY \n",
    "            w.writer_id\n",
    "    ),\n",
    "    writer_scores AS (\n",
    "        SELECT \n",
    "            w.movie_id,\n",
    "            COUNT(w.writer_id) AS writer_count,\n",
    "            AVG(COALESCE(was.writer_avg_score, 0.5)) AS writer_avg_score\n",
    "        FROM \n",
    "            writing w\n",
    "        LEFT JOIN \n",
    "            writer_avg_scores was ON w.writer_id = was.writer_id\n",
    "        GROUP BY \n",
    "            w.movie_id\n",
    "    ),\n",
    "    numbered_movies AS (\n",
    "        SELECT\n",
    "            m.*,\n",
    "            ROW_NUMBER() OVER () AS row_num\n",
    "        FROM \n",
    "            movies m\n",
    "    )\n",
    "    SELECT\n",
    "        nm.subset, \n",
    "        nm.movie_id,\n",
    "        nm.num_votes,\n",
    "        nm.runtime_min,\n",
    "        nm.title_length,\n",
    "        COALESCE(ds.director_avg_score, 0.5) AS director_avg_score,\n",
    "        COALESCE(ds.director_count, 0) AS director_count,\n",
    "        CASE WHEN nm.label THEN 1 ELSE 0 END AS label,\n",
    "        nm.label AS label_og,\n",
    "        COALESCE(ws.writer_avg_score, 0.5) AS writer_avg_score,\n",
    "        COALESCE(ws.writer_count, 0) AS writer_count\n",
    "    FROM \n",
    "        numbered_movies nm\n",
    "    LEFT JOIN \n",
    "        director_scores ds ON nm.movie_id = ds.movie_id\n",
    "    LEFT JOIN \n",
    "        writer_scores ws ON nm.movie_id = ws.movie_id\n",
    "    ORDER BY\n",
    "        nm.row_num;\n",
    "    ''').fetch_df()\n",
    "    con.close()\n",
    "    \n",
    "    \n",
    "    train = df[df['subset'] == 'train'].drop(['subset'], axis=1).dropna()\n",
    "    test = df[df['subset'] == 'test'].drop(['subset', 'label'], axis=1)\n",
    "    validation = df[df['subset'] == 'val'].drop(['subset', 'label'], axis=1)\n",
    "    \n",
    "    return train, test, validation\n",
    "\n",
    "def generate_pipeline(features: list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Function to generate the Spark pipeline based on the following operations:\n",
    "        - Assembling (choosing) the desired features (numeric).\n",
    "        - Index the selected features to be processed by the pipeline (strings).\n",
    "        - Initializing the pipeline based on the indexed features.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    pipeline = Pipeline().setStages([assembler])\n",
    "    return pipeline\n",
    "\n",
    "def generate_output_pipeline(features: list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Function to generate the Spark pipeline based on the following operations:\n",
    "        - Assembling (choosing) the desired features (numeric).\n",
    "        - Index the selected features to be processed by the pipeline (strings).\n",
    "        - Initializing the pipeline based on the indexed features.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    pipeline = Pipeline().setStages([assembler])\n",
    "    return pipeline\n",
    "\n",
    "def hyper_parameter_tuning(prepared: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Function to find the best hyperparameters for a RandomForestClassifier\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(rf.numTrees, [100, 200, 300])  # Number of trees in the forest\n",
    "                 .addGrid(rf.maxDepth, [2, 5, 10, 15])    # Maximum depth of each tree\n",
    "                 .addGrid(rf.maxBins, [5, 10, 20, 32])\n",
    "                 .build())\n",
    "    \n",
    "    # Define evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    \n",
    "    # Define cross-validation\n",
    "    cv = CrossValidator(estimator=rf,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)  # Use 5 folds\n",
    "    \n",
    "    # Train model using cross-validation\n",
    "    cv_model = cv.fit(prepared)\n",
    "    \n",
    "    # Best model from cross-validation\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    best_max_depth = best_model._java_obj.getMaxDepth()\n",
    "    best_num_trees = best_model._java_obj.getNumTrees()\n",
    "    best_max_bins = best_model._java_obj.getMaxBins()\n",
    "    print(\"Best maxDepth:\", best_max_depth)\n",
    "    print(\"Best numTrees:\", best_num_trees)\n",
    "    print(\"Best maxBins:\", best_max_bins)\n",
    "    \n",
    "def create_submission(model, validation, test, features) -> None:\n",
    "    \"\"\"\n",
    "    Create the required submission file in .csv format\n",
    "    \n",
    "    :param model: PySpark generated binary classifier\n",
    "    \"\"\"    \n",
    "    pipeline = generate_output_pipeline(features)\n",
    "    pipeline_fit = pipeline.fit(validation)\n",
    "    p_val = pipeline_fit.transform(validation)\n",
    "    p_test = pipeline_fit.transform(test)\n",
    "    \n",
    "    val_results = model.transform(p_val).select('prediction').toPandas()\n",
    "    test_results = model.transform(p_test).select('prediction').toPandas()\n",
    "\n",
    "    # Cast to bool and store in .csv\n",
    "    val_results.astype(bool).to_csv(\"val_result.csv\", index=False, header=None)\n",
    "    test_results.astype(bool).to_csv(\"test_result.csv\", index=False, header=None)\n",
    "\n",
    "    \n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main PySpark pipeline execution.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    # Initialize PySpark Context\n",
    "    conf = SparkConf().setAppName(\"binary-ml-classification\")\n",
    "    sc = SparkContext.getOrCreate(conf)\n",
    "    sqlContext = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Fetch data and process features to obtain a Spark Dataframe\n",
    "    train, test, validation = fetch_duckdb()\n",
    "    features = [\"runtime_min\", \"num_votes\", \"director_avg_score\", \"director_count\", \"writer_avg_score\", \"writer_count\"]\n",
    "    df_train = sqlContext.createDataFrame(train)\n",
    "    \n",
    "    # Generate the pipeline\n",
    "    pipeline = generate_pipeline(features)\n",
    "    \n",
    "    # Fit the pipeline using the Spark Dataframe\n",
    "    pipeline_fit = pipeline.fit(df_train)  \n",
    "    \n",
    "    # Generate and train the model\n",
    "    prepared = pipeline_fit.transform(df_train)\n",
    "\n",
    "    # Run if need to tune hyperparameters\n",
    "    # hyper_parameter_tuning(prepared)\n",
    "    \n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth = 2, numTrees=200, maxBins=10)\n",
    "    best_model = rf.fit(prepared)\n",
    "    \n",
    "    \n",
    "    # # Read output generation files\n",
    "    df_validation = sqlContext.createDataFrame(validation)\n",
    "    df_test = sqlContext.createDataFrame(test)\n",
    "\n",
    "    create_submission(best_model, df_validation, df_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigDataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
