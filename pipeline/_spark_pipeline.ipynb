{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Imports\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext, DataFrameWriter\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "\n",
    "# PySpark ML Imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml.feature import Bucketizer, VectorAssembler, StringIndexer\n",
    "\n",
    "# Other Imports\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# System paths\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Database path\n",
    "DATABASE_PATH = \"../database/DDBB_duckdb.duckdb\"\n",
    "\n",
    "\n",
    "def fetch_duckdb() -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetches all the required data from the database and returns an array of dataframes.\n",
    "    TEMP: Only data from the movies table is being fetched right now. Expand to writers\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(database=DATABASE_PATH, read_only=False)\n",
    "    df = con.execute('''\n",
    "        WITH director_avg_scores AS (\n",
    "        SELECT \n",
    "            d.director_id,\n",
    "            COALESCE(SUM(CASE WHEN m.label THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0.5) AS director_avg_score\n",
    "        FROM \n",
    "            directing d\n",
    "        INNER JOIN \n",
    "            movies m ON d.movie_id = m.movie_id\n",
    "        WHERE \n",
    "            m.subset = 'train'\n",
    "        GROUP BY \n",
    "            d.director_id\n",
    "    ),\n",
    "    director_scores AS (\n",
    "        SELECT \n",
    "            d.movie_id,\n",
    "            COUNT(d.director_id) AS director_count,\n",
    "            AVG(COALESCE(das.director_avg_score, 0.5)) AS director_avg_score\n",
    "        FROM \n",
    "            directing d\n",
    "        LEFT JOIN \n",
    "            director_avg_scores das ON das.director_id = d.director_id\n",
    "        GROUP BY \n",
    "            d.movie_id\n",
    "    ),\n",
    "    writer_avg_scores AS (\n",
    "        SELECT \n",
    "            w.writer_id,\n",
    "            COALESCE(SUM(CASE WHEN m.label THEN 1 ELSE 0 END) / NULLIF(COUNT(*), 0), 0.5) AS writer_avg_score\n",
    "        FROM \n",
    "            writing w\n",
    "        INNER JOIN \n",
    "            movies m ON w.movie_id = m.movie_id\n",
    "        WHERE \n",
    "            m.subset = 'train'\n",
    "        GROUP BY \n",
    "            w.writer_id\n",
    "    ),\n",
    "    writer_scores AS (\n",
    "        SELECT \n",
    "            w.movie_id,\n",
    "            COUNT(w.writer_id) AS writer_count,\n",
    "            AVG(COALESCE(was.writer_avg_score, 0.5)) AS writer_avg_score\n",
    "        FROM \n",
    "            writing w\n",
    "        LEFT JOIN \n",
    "            writer_avg_scores was ON w.writer_id = was.writer_id\n",
    "        GROUP BY \n",
    "            w.movie_id\n",
    "    )\n",
    "    SELECT\n",
    "        m.subset, \n",
    "        m.movie_id,\n",
    "        m.num_votes,\n",
    "        m.runtime_min,\n",
    "        m.title_length,\n",
    "        COALESCE(ds.director_avg_score, 0.5) AS director_avg_score,\n",
    "        COALESCE(ds.director_count, 0) AS director_count,\n",
    "        CASE WHEN m.label THEN 1 ELSE 0 END AS label,\n",
    "        COALESCE(ws.writer_avg_score, 0.5) AS writer_avg_score,\n",
    "        COALESCE(ws.writer_count, 0) AS writer_count\n",
    "    FROM \n",
    "        movies m\n",
    "    LEFT JOIN \n",
    "        director_scores ds ON m.movie_id = ds.movie_id\n",
    "    LEFT JOIN \n",
    "        writer_scores ws ON m.movie_id = ws.movie_id;\n",
    "    ''').fetch_df()\n",
    "    con.close()\n",
    "    \n",
    "    \n",
    "    train = df[df['subset'] == 'train'].drop(['subset'], axis=1).dropna()\n",
    "    test = df[df['subset'] == 'test'].drop(['subset', 'label'], axis=1)\n",
    "    validation = df[df['subset'] == 'val'].drop(['subset', 'label'], axis=1)\n",
    "    \n",
    "    return train, test, validation\n",
    "\n",
    "def generate_pipeline(features: list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Function to generate the Spark pipeline based on the following operations:\n",
    "        - Assembling (choosing) the desired features (numeric).\n",
    "        - Index the selected features to be processed by the pipeline (strings).\n",
    "        - Initializing the pipeline based on the indexed features.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    indexer = StringIndexer(inputCol=\"label\").setOutputCol(\"label-index\")\n",
    "    pipeline = Pipeline().setStages([assembler, indexer])\n",
    "    return pipeline\n",
    "\n",
    "def generate_output_pipeline(features: list) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Function to generate the Spark pipeline based on the following operations:\n",
    "        - Assembling (choosing) the desired features (numeric).\n",
    "        - Index the selected features to be processed by the pipeline (strings).\n",
    "        - Initializing the pipeline based on the indexed features.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    pipeline = Pipeline().setStages([assembler])\n",
    "    return pipeline\n",
    "\n",
    "    \n",
    "def create_submission(model, validation, test, features) -> None:\n",
    "    \"\"\"\n",
    "    Create the required submission file in .csv format\n",
    "    \n",
    "    :param model: PySpark generated binary classifier\n",
    "    \"\"\"    \n",
    "    pipeline = generate_output_pipeline(features)\n",
    "    pipeline_fit = pipeline.fit(validation)\n",
    "    p_val = pipeline_fit.transform(validation)\n",
    "    p_test = pipeline_fit.transform(test)\n",
    "    \n",
    "    val_results = model.transform(p_val).select('prediction').toPandas()\n",
    "    test_results = model.transform(p_test).select('prediction').toPandas()\n",
    "\n",
    "    # Cast to bool and store in .csv\n",
    "    val_results.astype(bool).to_csv(\"val_result.csv\", index=False, header=None)\n",
    "    test_results.astype(bool).to_csv(\"test_result.csv\", index=False, header=None)\n",
    "\n",
    "def automated_submission() -> None:\n",
    "    \"\"\"\n",
    "    Automates the submision of files to the Azure server for the competition\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    \n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main PySpark pipeline execution.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    # Initialize PySpark Context\n",
    "    conf = SparkConf().setAppName(\"binary-ml-classification\")\n",
    "    sc = SparkContext.getOrCreate(conf)\n",
    "    sqlContext = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Fetch data and process features to obtain a Spark Dataframe\n",
    "    train, test, validation = fetch_duckdb()\n",
    "    features = [\"runtime_min\", \"num_votes\", \"director_avg_score\",\t\"director_count\", \"writer_avg_score\",\"writer_count\"]\n",
    "    df_train = sqlContext.createDataFrame(train)\n",
    "    \n",
    "    # Generate the pipeline\n",
    "    pipeline = generate_pipeline(features)\n",
    "    \n",
    "    # Fit the pipeline using the Spark Dataframe\n",
    "    pipeline_fit = pipeline.fit(df_train)  \n",
    "    \n",
    "    # Generate and train the model\n",
    "    prepared = pipeline_fit.transform(df_train)\n",
    "    # dt = DecisionTreeClassifier(labelCol = \"label-index\", featuresCol= \"features\")\n",
    "    dt = RandomForestClassifier(labelCol=\"label-index\", featuresCol=\"features\")\n",
    "    dt_model = dt.fit(prepared)\n",
    "    \n",
    "    # Read output generation files\n",
    "    df_validation = sqlContext.createDataFrame(validation)\n",
    "    df_test = sqlContext.createDataFrame(test)\n",
    "\n",
    "    create_submission(dt_model, df_validation, df_test, features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigDataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
